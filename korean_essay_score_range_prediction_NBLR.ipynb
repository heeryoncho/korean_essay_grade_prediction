{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "korean_essay_score_range_prediction_NBLR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzJHRniu_wnb"
      },
      "source": [
        "2021 한국정보처리학회 춘계학술대회 발표 논문 \n",
        "\n",
        "조희련_1, 임현열_2, 차준우_1, 이유미_1 (1_중앙대학교 인문콘텐츠연구소, 2_중앙대학교 다빈치교양대학)\n",
        "\n",
        "\"KoBERT, 나이브 베이즈, 로지스틱 회귀의 한국어 쓰기 답안지 점수 구간 예측 성능 비교\"\n",
        "\n",
        "나이브 베이즈(Naive Bayes: NB)와 로지스틱 회귀(Logistic Regression) 실험 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xCkQJsbBCBi",
        "outputId": "2bad1c7f-cb37-4097-f1a6-8bba009a7853"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 213kB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHn2WjI2AyfY"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from konlpy.tag import Komoran\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7InhIC2QA7hi"
      },
      "source": [
        "def get_file(file_name):\n",
        "    with open(file_name) as f:\n",
        "        data = pd.read_csv(f, delimiter=\"\\t\", quotechar='\"')\n",
        "    return data\n",
        "\n",
        "def vectorize(train, val, test):\n",
        "    parser = Komoran()\n",
        "\n",
        "    temp_train = []\n",
        "    for doc in train:\n",
        "        temp_train.append(parser.morphs(doc))\n",
        "    result_train = [' '.join(tokens) for tokens in temp_train]\n",
        "\n",
        "    temp_val = []\n",
        "    for doc in val:\n",
        "        temp_val.append(parser.morphs(doc.replace(\"[[문단]] \",\"\")))\n",
        "    result_val = [' '.join(tokens) for tokens in temp_val]\n",
        "\n",
        "    temp_test = []\n",
        "    for doc in test:\n",
        "        temp_test.append(parser.morphs(doc))\n",
        "    result_test = [' '.join(tokens) for tokens in temp_test]\n",
        "\n",
        "    vect = CountVectorizer()\n",
        "    X_train = vect.fit_transform(result_train)\n",
        "    X_val = vect.transform(result_val)\n",
        "    X_test = vect.transform(result_test)\n",
        "\n",
        "    return X_train, X_val, X_test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHWc_2ohJRm9"
      },
      "source": [
        "### 실험 데이터는 아래의 URL에서 다운로드 받을 수 있습니다.\n",
        "\n",
        "http://aihumanities.org/ko/archive/data/?vid=1\n",
        "\n",
        "압축해제 하신 후 colab의 sample_data 폴더에 올려 주세요.\n",
        "\n",
        "이 때 폴더 구조와 파일의 위치(예시)는 다음과 같습니다.\n",
        "\n",
        "`sample_data/job/train_0.txt `\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1EmnprCFDD"
      },
      "source": [
        "# 코멘트 아웃하 나이브 베이즈와 로지스틱 회귀 실험 결과를 취득\n",
        "\n",
        "#clf = MultinomialNB()\n",
        "clf = LogisticRegression(random_state=0, max_iter=1000)\n",
        "\n",
        "folders = [\"job_plus_success\"]\n",
        "\n",
        "for folder in folders:\n",
        "    print(\"======================\")\n",
        "    print(\"result_{}\".format(folder))\n",
        "    print(\"======================\")\n",
        "    avg_acc_train = []\n",
        "    avg_acc_val = []\n",
        "    avg_acc_test = []    \n",
        "    for i in range(7):\n",
        "        train_data_file = \"sample_data/{}/train_{}.txt\".format(folder, i)\n",
        "        val_data_file = \"sample_data/{}/val_{}.txt\".format(folder, i)\n",
        "        test_data_file = \"sample_data/{}/test_{}.txt\".format(folder, i)\n",
        "\n",
        "        data_train = get_file(train_data_file)\n",
        "        train_doc = data_train[\"document\"].str.replace(\"[[문단]] \",\"\")\n",
        "        train_label = data_train[\"label\"]\n",
        "\n",
        "        data_val = get_file(val_data_file)\n",
        "        val_doc = data_val[\"document\"].str.replace(\"[[문단]] \",\"\")\n",
        "        val_label = data_val[\"label\"]\n",
        "\n",
        "        data_test = get_file(test_data_file)\n",
        "        test_doc = data_test[\"document\"].str.replace(\"[[문단]] \",\"\")\n",
        "        test_label = data_test[\"label\"]\n",
        "        X_train, X_val, X_test = vectorize(train_doc, val_doc, test_doc)\n",
        "\n",
        "\n",
        "        clf.fit(X_train, train_label)\n",
        "        pred_train = clf.predict(X_train)\n",
        "        pred_val = clf.predict(X_val)\n",
        "        pred_test = clf.predict(X_test)\n",
        "\n",
        "        '''\n",
        "        print(\"X_test\", X_test.shape)\n",
        "        print(\"y_test\", len(test_label))\n",
        "        print(\"X_val\", X_val.shape)\n",
        "        print(\"y_val\", len(val_label))\n",
        "        print(\"X_train\", X_train.shape)\n",
        "        print(\"y_train\", len(train_label))\n",
        "        '''\n",
        "\n",
        "        acc_train = accuracy_score(pred_train, train_label)\n",
        "        avg_acc_train.append(acc_train)\n",
        "\n",
        "        acc_val = accuracy_score(pred_val, val_label)\n",
        "        avg_acc_val.append(acc_val)\n",
        "\n",
        "        acc_test = accuracy_score(pred_test, test_label)\n",
        "        avg_acc_test.append(acc_test)\n",
        "\n",
        "        print(\"acc_train:\", round(acc_train, 5))\n",
        "        print(\"acc_val:\", round(acc_val, 5))\n",
        "        print(\"acc_test:\", round(acc_test, 5))\n",
        "        print(\"-------------------\")\n",
        "\n",
        "    avg_train = sum(avg_acc_train) / len(avg_acc_train)\n",
        "    avg_val = sum(avg_acc_val) / len(avg_acc_val)\n",
        "    avg_test = sum(avg_acc_test) / len(avg_acc_test)\n",
        "\n",
        "    print(\"AVG_TRAIN:\", round(avg_train, 5))\n",
        "    print(\"AVG_VAL:\", round(avg_val, 5))\n",
        "    print(\"AVG_TEST:\", round(avg_test, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LKaYuCWHXYv"
      },
      "source": [
        "### 로지스틱 회귀와 나이브 베이즈 모델 구축 시 사용된 특징 단어 확인 및 로지스틱 회귀에서의 각 클래스 별 특징 단어 상위 10위 표시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPIAJealZqn4"
      },
      "source": [
        "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
        "\n",
        "train_data_file = \"sample_data/job_plus_econ/train_6.txt\"\n",
        "data_train = get_file(train_data_file)\n",
        "train_doc = data_train[\"document\"].str.replace(\"[[문단]] \",\"\", regex=True)\n",
        "train_label = data_train[\"label\"]\n",
        "\n",
        "parser = Komoran()\n",
        "\n",
        "temp_train = []\n",
        "for doc in train_doc:\n",
        "    temp_train.append(parser.morphs(doc))\n",
        "result_train = [' '.join(tokens) for tokens in temp_train]\n",
        "\n",
        "vect = CountVectorizer()\n",
        "X_train = vect.fit_transform(result_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ChOt-cbIK9"
      },
      "source": [
        "sum_words = X_train.sum(axis=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzixYTUxbQiJ",
        "outputId": "0449a660-20ac-419a-b00a-1ebf744c889d"
      },
      "source": [
        "sum_words"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 3,  2,  1, ...,  1,  1, 30]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilgg4rICbUro"
      },
      "source": [
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bCv5lh5c2m6"
      },
      "source": [
        "words_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHhInDieNyfd",
        "outputId": "bb7a9a03-58ab-488f-b633-71cd07472760"
      },
      "source": [
        "len(words_freq)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQws1BwvOR-l"
      },
      "source": [
        "clf = LogisticRegression(random_state=0, max_iter=1000)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKerlfC6OTEV",
        "outputId": "f9fcbeaf-da6e-4bde-e484-587d3309c3cb"
      },
      "source": [
        "clf.fit(X_train, train_label)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i6c7nXZQTX2",
        "outputId": "bab13104-c1c9-4aad-c779-1349b16ed646"
      },
      "source": [
        "clf.classes_"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au7XvSbIOei1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efac4ee-5464-4dda-e268-3171dfe801f0"
      },
      "source": [
        "weight = clf.coef_\n",
        "weight"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01874652,  0.02591283, -0.00650765, ..., -0.01036387,\n",
              "         0.02606569, -0.12337049],\n",
              "       [ 0.04782676, -0.00297888,  0.03004586, ..., -0.00844004,\n",
              "        -0.00727872, -0.05334802],\n",
              "       [ 0.00200671, -0.01274985, -0.01345546, ...,  0.03086479,\n",
              "        -0.0139874 ,  0.26265692],\n",
              "       [-0.03108694, -0.01018411, -0.01008275, ..., -0.01206088,\n",
              "        -0.00479957, -0.0859384 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUDRrl8HQcNm"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-o4YmQROj7d"
      },
      "source": [
        "# https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
        "# 레이블이 '3'인 경우\n",
        "sel_weights = np.argsort(-weight[3])[:10]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXgiAO6LRQQd"
      },
      "source": [
        "vocab_idx = {y:x for x,y in vect.vocabulary_.items()}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byqRBPfTQ7LW",
        "outputId": "e2c68662-ddae-4ea4-fac4-ead63d2b4f9d"
      },
      "source": [
        "for w in sel_weights:\n",
        "    print(vocab_idx[w])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "직성\n",
            "가능\n",
            "발전\n",
            "아무리\n",
            "에서\n",
            "여유\n",
            "조건\n",
            "ㄴ다면\n",
            "지만\n",
            "그리고\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}